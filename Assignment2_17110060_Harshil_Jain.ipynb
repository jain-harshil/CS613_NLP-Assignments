{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2-17110060-Harshil_Jain.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "8ca1a04a-b871-4fdc-dced-e462caaea004",
        "id": "GQJIYi2AaREa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkyojBe5nrE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import re\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import operator\n",
        "import string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgupXbKJoCs0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "050d1941-b53a-48d0-c951-953e467f2041"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1zq1htKm75s",
        "colab_type": "text"
      },
      "source": [
        "# **DATA CLEANING AND PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFMHuTmPaTfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file1 = open(\"/content/drive/My Drive/speeches.txt\",\"r\")\n",
        "trainstr = file1.read()\n",
        "trainstr = trainstr.lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isfURB4wDGVO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4170431-ecc0-4a26-ffc9-0586c0bfda04"
      },
      "source": [
        "tnew = sent_tokenize(trainstr)\n",
        "for i in range (len(tnew)):\n",
        "  tnew[i] = re.sub(r'[^A-Za-z\\s\\']+', \"\", tnew[i])\n",
        "  tnew[i] = '<s> '+tnew[i]+' </s>'\n",
        "train = tnew[:13000] # 80% of dataset is train\n",
        "test = tnew[13000:] # 20% of dataset is test\n",
        "print(len(tnew))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr0krnfIn4kd",
        "colab_type": "text"
      },
      "source": [
        "# CLASSICAL APPROACH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YWchWc6ECRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fdist1 = {} #bigram\n",
        "for i in range (len(train)):\n",
        "  tokens = train[i].split()\n",
        "  bigrams = nltk.bigrams(tokens)\n",
        "  fdist = dict(nltk.FreqDist(bigrams))\n",
        "  fdist1 = dict(Counter(fdist)+Counter(fdist1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOW_g3eLwM0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fdist2 = {} #trigram\n",
        "for i in range (len(train)):\n",
        "  tokens = train[i].split()\n",
        "  trigrams = nltk.trigrams(tokens)\n",
        "  fdist = dict(nltk.FreqDist(trigrams))\n",
        "  fdist2 = dict(Counter(fdist)+Counter(fdist2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m98vjnEkOJFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import everygrams\n",
        "fdist3 = {} #quadgram\n",
        "for i in range (len(train)):\n",
        "  tokens = train[i].split()\n",
        "  fourgrams = list(everygrams(tokens,4,4)) \n",
        "  fdist = dict(nltk.FreqDist(fourgrams))\n",
        "  fdist3 = dict(Counter(fdist)+Counter(fdist3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EX6A7qjYP5a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1214c20e-e948-4717-e00c-3696bd5e9708"
      },
      "source": [
        "trainstr = re.sub(r'[^A-Za-z\\s\\']+', \"\", trainstr)\n",
        "ls = trainstr.split()\n",
        "dictfinal = {}\n",
        "for i in range (len(ls)):\n",
        "  if ls[i] not in dictfinal:\n",
        "    dictfinal[ls[i]] = 1\n",
        "  else:\n",
        "    dictfinal[ls[i]] += 1\n",
        "Vocab = len(dictfinal)\n",
        "Token = sum(dictfinal.values())\n",
        "Vocab"
      ],
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6069"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm05-Ebfq-Ol",
        "colab_type": "text"
      },
      "source": [
        "**MAXIMUM LIKELIHOOD ESTIMATIONS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWeAPKnIWwZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MLEunigram(w1):\n",
        "  if w1 not in d:\n",
        "    return 1/(Vocab+Token) # Add 1 Smoothing\n",
        "  else:\n",
        "    return (d[w1]+1)/(Vocab+Token) # Add 1 Smoothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqlCdMzhXF3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MLEbigram(w1,w2):\n",
        "  if (w1+\" \"+w2) not in fdist1:\n",
        "    if w1 in d.keys():\n",
        "      return 1/(d[w1]+Vocab) # Add 1 Smoothing\n",
        "    else:\n",
        "      return 1/(Vocab) # Add 1 Smoothing\n",
        "  else:\n",
        "    if w1 in d.keys():\n",
        "      return (fdist1(w1+\" \"+w2)+1)/(d[w1]+Vocab) # Add 1 Smoothing\n",
        "    else:\n",
        "      return (fdist1(w1+\" \"+w2)+1)/(Vocab) # Add 1 Smoothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DQTi2CeXGg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MLEtrigram(w1,w2,w3):\n",
        "  if (w1+\" \"+w2+\" \"+w3) not in fdist2:\n",
        "    if w1+\" \"+w2 in fdist1.keys():\n",
        "      return 1/(fdist1[w1+\" \"+w2]+Vocab) # Add 1 Smoothing\n",
        "    else:\n",
        "      return 1/(Vocab) # Add 1 Smoothing\n",
        "  else:\n",
        "    if w1+\" \"+w2 in fdist1.keys():\n",
        "      return (fdist2(w1+\" \"+w2+\" \"+w3)+1)/(fdist1[w1+\" \"+w2]+Vocab) # Add 1 Smoothing\n",
        "    else:\n",
        "      return (fdist2(w1+\" \"+w2+\" \"+w3)+1)/(Vocab) # Add 1 Smoothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOrP6d5DRROK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MLEquadgram(w1,w2,w3,w4):\n",
        "  if (w1+\" \"+w2+\" \"+w3+\" \"+w4) not in fdist3:\n",
        "    if w1+\" \"+w2 + \" \"+w3 in fdist2.keys():\n",
        "      return 1/(fdist2[w1+\" \"+w2+\" \"+w3]+Vocab) # Add 1 Smoothing\n",
        "    else:\n",
        "      return 1/(Vocab) # Add 1 Smoothing\n",
        "  else:\n",
        "    if w1+\" \"+w2 + \" \"+w3 in fdlist2.keys():\n",
        "      return (fdist3(w1+\" \"+w2+\" \"+w3+\" \"+w4)+1)/(fdist2[w1+\" \"+w2+\" \"+w3]+Vocab) # Add 1 Smoothing\n",
        "    else:\n",
        "      return (fdist3(w1+\" \"+w2+\" \"+w3+\" \"+w4)+1)/(Vocab) # Add 1 Smoothing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huDypeZCpsdY",
        "colab_type": "text"
      },
      "source": [
        "# CALCULATING SENTENCE PROBABILITIES USING UNIGRAM, BIGRAM, TRIGRAM AND QUADGRAM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3JKfyKfyiH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unigramsentenceprob(sentence):\n",
        "  sentence_probability_log_sum = 0\n",
        "  for word in sentence:\n",
        "    x = MLEunigram(word)\n",
        "    sentence_probability_log_sum += math.log(x,2)\n",
        "  return math.pow(2, sentence_probability_log_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oF5CFZC57f9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bigramsentenceprob(sentence):\n",
        "  bigram_sentence_probability_log_sum = 0\n",
        "  previous_word = None\n",
        "  for word in sentence:\n",
        "    if previous_word!=None:\n",
        "      x = MLEbigram(previous_word,word)\n",
        "      bigram_sentence_probability_log_sum += math.log(x,2)\n",
        "    previous_word = word\n",
        "  return math.pow(2, bigram_sentence_probability_log_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqyC4n-oDnEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trigramsentenceprob(sentence):\n",
        "  trigram_sentence_probability_log_sum = 0\n",
        "  previous_word = None\n",
        "  previous_previous_word = None\n",
        "  for word in sentence:\n",
        "    if previous_word!=None and previous_previous_word!=None:\n",
        "      x = MLEtrigram(previous_previous_word,previous_word,word)\n",
        "      trigram_sentence_probability_log_sum += math.log(x,2)\n",
        "    previous_previous_word = previous_word\n",
        "    previous_word = word\n",
        "  return math.pow(2, trigram_sentence_probability_log_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MIuewpuDn-4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quadgramsentenceprob(sentence):\n",
        "  quadgram_sentence_probability_log_sum = 0\n",
        "  previous_word = None\n",
        "  previous_previous_word = None\n",
        "  previous_previous_previous_word = None\n",
        "  for word in sentence:\n",
        "    if previous_word!=None and previous_previous_word!=None and previous_previous_previous_word!=None :\n",
        "      x = MLEquadgram(previous_previous_previous_word,_previous_previous_word,previous_word,word)\n",
        "      quadgram_sentence_probability_log_sum += math.log(x,2)\n",
        "    previous_previous_previous_word = previous_previous_word\n",
        "    previous_previous_word = previous_word\n",
        "    previous_word = word\n",
        "  return math.pow(2, quadgram_sentence_probability_log_sum)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvaDwbCe_CmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_number_of_unigrams(sentences):\n",
        "  unigram_count = 0\n",
        "  for sentence in sentences:\n",
        "    # remove two for <s> and </s>\n",
        "    unigram_count += len(sentence) - 2\n",
        "  return unigram_count\n",
        "def calculate_number_of_bigrams(sentences):\n",
        "  bigram_count = 0\n",
        "  for sentence in sentences:\n",
        "    bigram_count += len(sentence) - 1\n",
        "  return bigram_count\n",
        "def calculate_number_of_trigrams(sentences):\n",
        "  trigram_count = 0\n",
        "  for sentence in sentences:\n",
        "    trigram_count += len(sentence) - 2\n",
        "  return trigram_count\n",
        "def calculate_number_of_quadgrams(sentences):\n",
        "  quadgram_count = 0\n",
        "  for sentence in sentences:\n",
        "    quadgram_count += len(sentence) - 3\n",
        "  return quadgram_count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDxr1huAp6kz",
        "colab_type": "text"
      },
      "source": [
        "# FUNCTIONS FOR CALCULATING PERPLEXITY FOR UNIGRAM, BIGRAM AND TRIGRAM MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKmv2uVA-vtl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_unigram_perplexity(sentences):\n",
        "  unigram_count = calculate_number_of_unigrams(sentences)\n",
        "  sentence_probability_log_sum = 0\n",
        "  for sentence in sentences:\n",
        "    try:\n",
        "      sentence_probability_log_sum -= math.log(unigramsentenceprob(sentence), 2)\n",
        "    except:\n",
        "      sentence_probability_log_sum -= 0\n",
        "  return math.pow(2, sentence_probability_log_sum / unigram_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeQSfSO_IyW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_bigram_perplexity(sentences):\n",
        "  bigram_count = calculate_number_of_bigrams(sentences)\n",
        "  sentence_probability_log_sum = 0\n",
        "  for sentence in sentences:\n",
        "    try:\n",
        "      sentence_probability_log_sum -= math.log(bigramsentenceprob(sentence), 2)\n",
        "    except:\n",
        "      sentence_probability_log_sum -= 0\n",
        "  return math.pow(2, sentence_probability_log_sum / bigram_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9TQLRI6IzzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_trigram_perplexity(sentences):\n",
        "  trigram_count = calculate_number_of_bigrams(sentences)\n",
        "  sentence_probability_log_sum = 0\n",
        "  for sentence in sentences:\n",
        "    try:\n",
        "      sentence_probability_log_sum -= math.log(trigramsentenceprob(sentence), 2)\n",
        "    except:\n",
        "      sentence_probability_log_sum -= 0\n",
        "  return math.pow(2, sentence_probability_log_sum / trigram_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAnA4iCpI0U3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_quadgram_perplexity(sentences):\n",
        "  quadgram_count = calculate_number_of_quadgrams(sentences)\n",
        "  sentence_probability_log_sum = 0\n",
        "  for sentence in sentences:\n",
        "    try:\n",
        "      sentence_probability_log_sum -= math.log(quadgramsentenceprob(sentence), 2)\n",
        "    except:\n",
        "      sentence_probability_log_sum -= 0\n",
        "  return math.pow(2, sentence_probability_log_sum / quadgram_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZTn6KS1qZMC",
        "colab_type": "text"
      },
      "source": [
        "# PERPLEXITY OF VARIOUS MODELS WITH CLASSICAL APPROACH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE7All7zJOCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "e4ad662a-8e9c-4d87-bbe0-2059b4e5099c"
      },
      "source": [
        "import math\n",
        "print(\"Perplexity of test corpus with respect to unigram model is:\",end = \" \")\n",
        "print(calculate_unigram_perplexity(test))\n",
        "print(\"Perplexity of test corpus with respect to bigram model is:\",end = \" \")\n",
        "print(calculate_bigram_perplexity(test))\n",
        "print(\"Perplexity of test corpus with respect to trigram model is:\",end = \" \")\n",
        "print(calculate_trigram_perplexity(test))\n",
        "print(\"Perplexity of test corpus with respect to quadgram model is:\",end = \" \")\n",
        "print(calculate_quadgram_perplexity(test))\n"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of test corpus with respect to unigram model is: 186.6357508882258\n",
            "Perplexity of test corpus with respect to bigram model is: 147.40810983034604\n",
            "Perplexity of test corpus with respect to trigram model is: 135.92047675682477\n",
            "Perplexity of test corpus with respect to quadgram model is: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnZAybsvqrHL",
        "colab_type": "text"
      },
      "source": [
        "# CLASSICAL APPROACH FOR FUNCTION GENERATOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6SBm8s6s2uA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nc(sentences, ng):\n",
        "  counterdict = dict()\n",
        "  n = 0\n",
        "  for i in range(len(sentences)):\n",
        "    tokens = sentences[i].split(\" \")\n",
        "    for j in range(0,len(tokens)-ng+1):\n",
        "      k = tuple(tokens[j: j+ng])\n",
        "      if k not in counterdict:\n",
        "        counterdict[k] = 1\n",
        "        n += 1\n",
        "      else:\n",
        "        counterdict[k] += 1\n",
        "  return [counterdict, n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOV_ix16sDdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cu, nu= nc(tnew, 1) # unigram\n",
        "cb, nb = nc(tnew, 2) # bigram\n",
        "ct, nt = nc(tnew, 3) # trigram\n",
        "cq, nq = nc(tnew, 4) # quadgram\n",
        "\n",
        "def uni(word_list):\n",
        "  prob = 0\n",
        "  try:\n",
        "    num = cu[tuple(word_list)]\n",
        "    return num/nu\n",
        "  except:\n",
        "    return prob\n",
        "def bi(word_list):\n",
        "  prob = 0\n",
        "  try:\n",
        "    num = cb[tuple(word_list)]\n",
        "    del word_list[-1]\n",
        "    den = cu[tuple(word_list)]\n",
        "    return num/den\n",
        "  except:\n",
        "    return prob\n",
        "def tri(word_list):\n",
        "  prob = 0\n",
        "  try:\n",
        "    num = ct[tuple(word_list)]\n",
        "    del word_list[-1]\n",
        "    den = cb[tuple(word_list)]\n",
        "    return num/den\n",
        "  except:\n",
        "    return prob\n",
        "def quad(word_list):\n",
        "  prob = 0\n",
        "  try:\n",
        "    num = cq[tuple(word_list)]\n",
        "    del word_list[-1]\n",
        "    den = ct[tuple(word_list)]\n",
        "    return num/den\n",
        "  except:\n",
        "    return prob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksyaJzjDVvDW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classical generator to generate new words using a n_gram model\n",
        "def Generator(n_gram, initseq):\n",
        "  sentence = []\n",
        "  if(n_gram==1):\n",
        "    for i in range(30):\n",
        "      mp = 0\n",
        "      mpl = list()\n",
        "      for j in dictfinal.keys():\n",
        "        k = (uni([j]))\n",
        "        if(k>mp):\n",
        "          mp = k\n",
        "          mpl = [j]\n",
        "        elif(k==mp):\n",
        "          max_prob_list.append(j)\n",
        "      samples = np.random.multinomial(30,[mp]*len(mpl),size=1)\n",
        "      index, value = max(enumerate(samples), key = operator.itemgetter(1))\n",
        "      sentence.append(mpl[index])\n",
        "  else:\n",
        "    sentence.extend(initseq)\n",
        "    i = len(initseq)\n",
        "    while(sentence[-1]!=\"</s>\" and i<30):\n",
        "      mp = 0\n",
        "      mpl = list()\n",
        "      for j in dictfinal.keys():\n",
        "        word_list = sentence[(1-n_gram):]\n",
        "        word_list.append(j)\n",
        "        z1 = bi(word_list)\n",
        "        z2 = tri(word_list)\n",
        "        z3 = quad(word_list)\n",
        "        if(n_gram==2):\n",
        "          k = z1\n",
        "        elif(n_gram==3):\n",
        "          k = z2\n",
        "        elif(n_gram==4):\n",
        "          k = z3\n",
        "        if(k>mp):\n",
        "          mp = k\n",
        "          mpl = [j]\n",
        "        elif(k==mp):\n",
        "          mpl.append(j)\n",
        "      t = mp\n",
        "      inp = [t]*len(mpl)\n",
        "      samples = np.random.multinomial(30,inp,size=1)\n",
        "      index, value = max(enumerate(samples), key = operator.itemgetter(1))\n",
        "      sentence.append(mpl[index])\n",
        "      i = i+1\n",
        "    \n",
        "  return [\"<s>\"]+sentence+[\"</s>\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeXuaCCG8x_h",
        "colab_type": "text"
      },
      "source": [
        "# RANDOM SENTENCES GENERATED USING UNIGRAM, BIGRAM, TRIGRAM AND QUADGRAM MODELS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phx-8HjoW4Uo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "f33d8c40-d512-486e-de52-eca7436b0bb1"
      },
      "source": [
        "print(\"Random Text generated by Unigram Model: \",end = \"\")\n",
        "print(\" \".join(Generator(1,[\"he\"])))\n",
        "print(\"Random Text generated by Bigram Model: \",end = \"\")\n",
        "print(\" \".join(Generator(2,[\"it\", \"should\"])))\n",
        "print(\"Random Text generated by Trigram Model: \",end = \"\")\n",
        "print(\" \".join(Generator(3,[\"there\", \"is\", \"always\"])))\n",
        "print(\"Random Text generated by Quadgram Model: \",end = \"\")\n",
        "print(\" \".join(Generator(4,[\"it\", \"should\", \"not\", \"be\"])))"
      ],
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Text generated by Unigram Model: <s> the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the </s>\n",
            "Random Text generated by Bigram Model: <s> it should have to be a lot of the way i dont know what i dont know what i dont know what i dont know what i dont know what </s>\n",
            "Random Text generated by Trigram Model: <s> there is always again to bring our jobs back from china from mexico and i said i dont know what theyre doing it because they want to be a great </s>\n",
            "Random Text generated by Quadgram Model: <s> it should not be allowed to run i would be very nice as far as im concerned i really dont know much about it thank thank thank thank thank thank </s>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9VxCP_DRda2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "21d5da25-215d-498a-e381-1941214940bc"
      },
      "source": [
        "print(\"The above 4 sentences generated by various approaches are not gramatically correct\")"
      ],
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The above 4 sentences generated by various approaches are not gramatically correct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qb2HwUU8qePW",
        "colab_type": "text"
      },
      "source": [
        "# NEURAL APPROACH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kt4lgeIs5mIS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Neural Approach\n",
        "\n",
        "sequences = list()\n",
        "n_gram = 3\n",
        "for i in range(len(train)):\n",
        "  tokens = train[i].split(\" \")\n",
        "  if(len(tokens)>=n_gram):\n",
        "    for k in range(0,len(tokens)-n_gram+1):\n",
        "      sequences.append(tokens[k:k+n_gram])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdiiA6XX7D6z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "72ff30bf-b7bb-4842-ac86-aeb56b870c82"
      },
      "source": [
        "sequences[:10]"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['<s>', 'thank', 'you'],\n",
              " ['thank', 'you', 'so'],\n",
              " ['you', 'so', 'much'],\n",
              " ['so', 'much', '</s>'],\n",
              " ['<s>', \"that's\", 'so'],\n",
              " [\"that's\", 'so', 'nice'],\n",
              " ['so', 'nice', '</s>'],\n",
              " ['<s>', \"isn't\", 'he'],\n",
              " [\"isn't\", 'he', 'a'],\n",
              " ['he', 'a', 'great']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcOHWJAB7SrJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lkA_Ewj67Kke",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "integer_encoded_sequences = tokenizer.texts_to_sequences(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJM4DX0a7NTL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66646652-c1ae-4465-8a8a-b64023934a75"
      },
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5481\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkMUiYn87aCl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "5fece0b5-28ff-4e9c-c9e0-950e79938bab"
      },
      "source": [
        "integer_encoded_sequences[:10]"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 137, 10],\n",
              " [137, 10, 20],\n",
              " [10, 20, 96],\n",
              " [20, 96, 4],\n",
              " [3, 610, 20],\n",
              " [610, 20, 181],\n",
              " [20, 181, 4],\n",
              " [3, 2354, 35],\n",
              " [2354, 35, 7],\n",
              " [35, 7, 49]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QenYHhZj7bXY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b4366eb9-e0d9-46e4-8204-e49dc031f547"
      },
      "source": [
        "integer_encoded_sequences = np.array(integer_encoded_sequences)\n",
        "np.shape(sequences)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(133651, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj-Ornte7hEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = integer_encoded_sequences[:,:-1]\n",
        "y = integer_encoded_sequences[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-kfhZCQ7n23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEo9-oScqilo",
        "colab_type": "text"
      },
      "source": [
        "# VANILLA RNN MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fId33u5y7qfG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "fe7da373-4ecc-4623-d1fa-53411c43ba24"
      },
      "source": [
        "# Building Vanilla RNN Model\n",
        "from keras.layers import SimpleRNN\n",
        "\n",
        "rnn_model = Sequential()\n",
        "rnn_model.add(Embedding(vocab_size, 50, input_length=n_gram-1))\n",
        "rnn_model.add(SimpleRNN(35, return_sequences=True))\n",
        "rnn_model.add(SimpleRNN(35))\n",
        "rnn_model.add(Dense(60, activation='relu'))\n",
        "rnn_model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "rnn_model.summary()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (None, 2, 50)             274050    \n",
            "_________________________________________________________________\n",
            "simple_rnn_5 (SimpleRNN)     (None, 2, 35)             3010      \n",
            "_________________________________________________________________\n",
            "simple_rnn_6 (SimpleRNN)     (None, 35)                2485      \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 60)                2160      \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 5481)              334341    \n",
            "=================================================================\n",
            "Total params: 616,046\n",
            "Trainable params: 616,046\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4deFQrv7vJ1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "433e7f1c-5b45-488d-b50d-7aa2708b3a9b"
      },
      "source": [
        "rnn_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "rnn_model.fit(X, y, batch_size=128, epochs=20)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "133651/133651 [==============================] - 21s 154us/step - loss: 5.6176 - acc: 0.1354\n",
            "Epoch 2/20\n",
            "133651/133651 [==============================] - 18s 137us/step - loss: 4.8511 - acc: 0.1907\n",
            "Epoch 3/20\n",
            "133651/133651 [==============================] - 20s 149us/step - loss: 4.5738 - acc: 0.2106\n",
            "Epoch 4/20\n",
            "133651/133651 [==============================] - 19s 144us/step - loss: 4.3761 - acc: 0.2241\n",
            "Epoch 5/20\n",
            "133651/133651 [==============================] - 18s 137us/step - loss: 4.2148 - acc: 0.2348\n",
            "Epoch 6/20\n",
            "133651/133651 [==============================] - 19s 145us/step - loss: 4.0806 - acc: 0.2445\n",
            "Epoch 7/20\n",
            "133651/133651 [==============================] - 19s 144us/step - loss: 3.9622 - acc: 0.2534\n",
            "Epoch 8/20\n",
            "133651/133651 [==============================] - 19s 138us/step - loss: 3.8565 - acc: 0.2615\n",
            "Epoch 9/20\n",
            "133651/133651 [==============================] - 20s 149us/step - loss: 3.7616 - acc: 0.2697\n",
            "Epoch 10/20\n",
            "133651/133651 [==============================] - 19s 145us/step - loss: 3.6769 - acc: 0.2775\n",
            "Epoch 11/20\n",
            "133651/133651 [==============================] - 18s 138us/step - loss: 3.6037 - acc: 0.2842\n",
            "Epoch 12/20\n",
            "133651/133651 [==============================] - 19s 146us/step - loss: 3.5372 - acc: 0.2906\n",
            "Epoch 13/20\n",
            "133651/133651 [==============================] - 20s 147us/step - loss: 3.4790 - acc: 0.2967\n",
            "Epoch 14/20\n",
            "133651/133651 [==============================] - 18s 138us/step - loss: 3.4261 - acc: 0.3032\n",
            "Epoch 15/20\n",
            "133651/133651 [==============================] - 19s 145us/step - loss: 3.3830 - acc: 0.3088\n",
            "Epoch 16/20\n",
            "133651/133651 [==============================] - 20s 147us/step - loss: 3.3428 - acc: 0.3133\n",
            "Epoch 17/20\n",
            "133651/133651 [==============================] - 19s 139us/step - loss: 3.3060 - acc: 0.3176\n",
            "Epoch 18/20\n",
            "133651/133651 [==============================] - 19s 143us/step - loss: 3.2733 - acc: 0.3223\n",
            "Epoch 19/20\n",
            "133651/133651 [==============================] - 20s 150us/step - loss: 3.2451 - acc: 0.3259\n",
            "Epoch 20/20\n",
            "133651/133651 [==============================] - 18s 137us/step - loss: 3.2164 - acc: 0.3292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0e71b626a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvk7gUmLQfFb",
        "colab_type": "text"
      },
      "source": [
        "# GENERATING SENTENCES ON TEST CORPUS USING RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb7BUbZlD1B7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "2e640653-3229-4859-9266-c95325efb222"
      },
      "source": [
        "generated_rnn = []\n",
        "for i in range(len(test)):\n",
        "  result = list()\n",
        "  text = test[i].split(\" \")\n",
        "  f = 0\n",
        "  if(len(text)>=n_gram):\n",
        "    result = text[:n_gram-1]\n",
        "    l = n_gram\n",
        "    while(result[-1]!=\"</s>\"):\n",
        "      encoded = tokenizer.texts_to_sequences([result[-n_gram+1:]])\n",
        "      encoded = np.array(encoded)\n",
        "      yhat = rnn_model.predict_classes(encoded, verbose=0)\n",
        "      out_word = ''\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "        if index == yhat:\n",
        "          out_word = word\n",
        "          break\n",
        "      result.append(out_word)\n",
        "      l+=1\n",
        "    generated_rnn.append(\" \".join(result))\n",
        "generated_rnn[0:5]"
      ],
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s> so i dont know </s>',\n",
              " '<s> thats what i do </s>',\n",
              " '<s> thats what i do </s>',\n",
              " '<s> ive been a disaster </s>',\n",
              " '<s> all of the united states </s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXDra9m0RMxM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d09fef8c-2fa2-437a-923d-6ccb9ca4c9d5"
      },
      "source": [
        "print(\"The above 5 sentences are not gramatically and semantically correct\")"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The above 5 sentences are not gramatically and semantically correct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAoKNIklqmMX",
        "colab_type": "text"
      },
      "source": [
        "# LSTM MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLoZu6l_9MTe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "e7c87c6a-8480-433c-8e72-69c16d0394c2"
      },
      "source": [
        "# Long Short Term Memory Model\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(vocab_size, 50, input_length=n_gram-1))\n",
        "lstm_model.add(LSTM(35, return_sequences=True))\n",
        "lstm_model.add(LSTM(35))\n",
        "lstm_model.add(Dense(55, activation='relu'))\n",
        "lstm_model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "lstm_model.summary()"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 2, 50)             274050    \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 2, 35)             12040     \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 35)                9940      \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 55)                1980      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 5481)              306936    \n",
            "=================================================================\n",
            "Total params: 604,946\n",
            "Trainable params: 604,946\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yn4R3bd9RU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "1a3b39b3-e462-4258-eebc-26c4e26f419e"
      },
      "source": [
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "lstm_model.fit(X, y, batch_size=128, epochs=20)"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "133651/133651 [==============================] - 32s 238us/step - loss: 6.1052 - acc: 0.0970\n",
            "Epoch 2/20\n",
            "133651/133651 [==============================] - 28s 208us/step - loss: 5.7382 - acc: 0.1042\n",
            "Epoch 3/20\n",
            "133651/133651 [==============================] - 30s 223us/step - loss: 5.3132 - acc: 0.1490\n",
            "Epoch 4/20\n",
            "133651/133651 [==============================] - 28s 213us/step - loss: 5.0113 - acc: 0.1750\n",
            "Epoch 5/20\n",
            "133651/133651 [==============================] - 27s 204us/step - loss: 4.8037 - acc: 0.1912\n",
            "Epoch 6/20\n",
            "133651/133651 [==============================] - 28s 206us/step - loss: 4.6463 - acc: 0.2023\n",
            "Epoch 7/20\n",
            "133651/133651 [==============================] - 27s 202us/step - loss: 4.5131 - acc: 0.2116\n",
            "Epoch 8/20\n",
            "133651/133651 [==============================] - 27s 204us/step - loss: 4.3924 - acc: 0.2193\n",
            "Epoch 9/20\n",
            "133651/133651 [==============================] - 29s 221us/step - loss: 4.2854 - acc: 0.2263\n",
            "Epoch 10/20\n",
            "133651/133651 [==============================] - 29s 217us/step - loss: 4.1885 - acc: 0.2347\n",
            "Epoch 11/20\n",
            "133651/133651 [==============================] - 29s 218us/step - loss: 4.1039 - acc: 0.2399\n",
            "Epoch 12/20\n",
            "133651/133651 [==============================] - 29s 215us/step - loss: 4.0261 - acc: 0.2457\n",
            "Epoch 13/20\n",
            "133651/133651 [==============================] - 28s 209us/step - loss: 3.9547 - acc: 0.2506\n",
            "Epoch 14/20\n",
            "133651/133651 [==============================] - 30s 224us/step - loss: 3.8878 - acc: 0.2554\n",
            "Epoch 15/20\n",
            "133651/133651 [==============================] - 29s 216us/step - loss: 3.8260 - acc: 0.2611\n",
            "Epoch 16/20\n",
            "133651/133651 [==============================] - 29s 218us/step - loss: 3.7684 - acc: 0.2663\n",
            "Epoch 17/20\n",
            "133651/133651 [==============================] - 29s 219us/step - loss: 3.7157 - acc: 0.2717\n",
            "Epoch 18/20\n",
            "133651/133651 [==============================] - 31s 229us/step - loss: 3.6661 - acc: 0.2766\n",
            "Epoch 19/20\n",
            "133651/133651 [==============================] - 29s 215us/step - loss: 3.6213 - acc: 0.2809\n",
            "Epoch 20/20\n",
            "133651/133651 [==============================] - 30s 224us/step - loss: 3.5793 - acc: 0.2850\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0eb0734208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O6ILhZvQXob",
        "colab_type": "text"
      },
      "source": [
        "# GENERATING SENTENCES ON TEST CORPUS USING LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weUFQuOjPgDO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "8cc5e109-80b0-483f-b2d6-2068a7a01d86"
      },
      "source": [
        "generated_lstm = []\n",
        "for i in range(len(test)):\n",
        "  result = list()\n",
        "  text = test[i].split(\" \")\n",
        "  f = 0\n",
        "  if(len(text)>=n_gram):\n",
        "    result = text[:n_gram-1]\n",
        "    l = n_gram\n",
        "    while(result[-1]!=\"</s>\"):\n",
        "      encoded = tokenizer.texts_to_sequences([result[-n_gram+1:]])\n",
        "      encoded = np.array(encoded)\n",
        "      yhat = lstm_model.predict_classes(encoded, verbose=0)\n",
        "      out_word = ''\n",
        "      for word, index in tokenizer.word_index.items():\n",
        "        if index == yhat:\n",
        "          out_word = word\n",
        "          break\n",
        "      result.append(out_word)\n",
        "      l+=1\n",
        "    generated_lstm.append(\" \".join(result))\n",
        "generated_lstm[0:5]"
      ],
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s> so i said to the united states </s>',\n",
              " '<s> thats what we have to do it </s>',\n",
              " '<s> thats what we have to do it </s>',\n",
              " '<s> ive been a very very good </s>',\n",
              " '<s> all of the united states </s>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ysIKjmuRVDp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9593ce45-3c64-49a6-d422-bb0a33cacd5b"
      },
      "source": [
        "print(\"The above 5 sentences are not gramatically and semantically correct\")"
      ],
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The above 5 sentences are not gramatically and semantically correct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7UI9pbYPkXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9e3b7be6-e4c4-4675-b638-6b7b6e2bb19c"
      },
      "source": [
        "print(\"Perplexity of RNN on test corpus for trigram model is:\",end = \"\")\n",
        "print(print(calculate_trigram_perplexity(rnn_generated)))\n",
        "\n",
        "print(\"Perplexity of LSTM on test corpus for trigram model is:\",end = \"\")\n",
        "print(print(calculate_trigram_perplexity(lstm_generated)))"
      ],
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of RNN on test corpus for trigram model is:501.4042458789\n",
            "Perplexity of LSTM on test corpus for trigram model is: 421.789546212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhR7wI1iQV9A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b9a5974-30ec-4ce0-c352-8a839b24c1df"
      },
      "source": [
        "print(\"Perplexity of test corpus with respect to trigram model by classical approach is:\",end = \" \")\n",
        "print(calculate_trigram_perplexity(test))"
      ],
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Perplexity of test corpus with respect to trigram model by classical approach is: 135.92047675682477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrXIIxvGrf7C",
        "colab_type": "text"
      },
      "source": [
        "# Does Neural performs better than Classical, if so, why? If not, why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZlK3TrHrlTz",
        "colab_type": "text"
      },
      "source": [
        "**Answer:** **As far as perplexity is concerned, LSTM performs better than RNN. ~H~owever perplexity values for neural approaches are higher than the classical approaches. This shows that the classical approaches are performing better. Grammatical sense is better in classical approaches. But neural approaches are able to generalize better bu semantic sense is worse. This can take in general n-1 words and predict the nth word which leads to less semantuc sense. However for sentences, it takes all previous words to produce the output and so more semantucally correct sentences are produced at the output.**"
      ]
    }
  ]
}